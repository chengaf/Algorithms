---
title: 优化器
date: 2019-10-09 13:14:36
tags: [深度学习,机器学习]
mathjax: true
toc: true
---

内容提要
> 1. Batch Gradient Descent (BGD)批梯度下降
> 2. Stochastic Gradient Descent (SGD)
> 3. Mini-Batch Gradient Descent(MBGD)
> 4. 改进措施: 
>   1. 动量Momentum的SGD; 
>   2. 牛顿加速梯度 Nesterov Accelerated Gradient; 
>   3. AdaDelta算法; 
>   4. Adam算法 Adaptive Moment Estimate
> 5. 总结

<!--more-->

大概说明：每次更新多少参数、是否能获得最优解（全局最小值）、优缺点说明。

## 1. Batch Gradient Descent (BGD)批梯度下降

  - 每次更新时，利用整个训练数据计算Loss对参数的梯度:
    $$\theta = \theta-\eta\nabla_{\theta}J(\theta)$$
- 特点: 
  - 一次更新中，对整个数据集计算梯度（对于很大数据集来讲可能会有相似样本，计算题都时可能会有冗余），所以计算起来慢，无法适应大数据集且无法投入新的数据实时更新模型；
  - 对于凸函数可以收敛到全局最小值，对于非凸函数可以收敛到局部最小值。

## 2. Stochastic Gradient Descent (SGD)

  - 每次更新时，对每个样本进行梯度更新：
$$\theta=\theta-\eta\nabla_{\theta}J(\theta;x^{(i)};y^{(i)})$$
  - 特点：
     - 如果样本量大的话相对BGD可能只用部分样本就已经达到最优；
     - SGD噪音比BGD要多，使得SGD并不是每次迭代都向着整体最优化方向，虽然训练速度快，但是准确度下降，并不是全局最优。虽然包含一定随机性，但是从期望上来看，它是等于正确的导数的；
     - SGD更新比较频繁，会造成Loss的严重震荡；
     - 可获得局部最优解

## 3. Mini-Batch Gradient Descent(MBGD)

  - 每次更新时，对一小批量样本（n个）进行更新：
$$\theta=\theta-\eta\nabla J(\theta;x^{(i:i+n)};y^{(i:i+n)})$$
  - 特点：
     - 降低了更新时的方差，收敛更稳定
     - 可以充分利用矩阵操作来进行更有效的梯度计算
     - 缺点1，不能保证很好的收敛性，learning rate太小收敛速度很慢，太大loss function会在极值处不停地震荡甚至偏离（可设置动态学习率）。

<p style="background-color:mediumseagreen;"> BGD、SGD以及MBGD都将面临的问题体</p>
  - <p style="background-color:mediumseagreen;">&#x2003;问题一：对于非凸函数要避免陷入局部极小值或者鞍点处，因为鞍点周围的error是一样的，所有维度的梯度都接近于0，SGD很容易困在这里。（会在局部最小值处和鞍点处震荡跳动，因为在此点处，如果训练集全集带入即BGD则优化会停止不动，如果是mini-batch或者SGD，每次找到的梯度都是不同的，就会发生震荡，来回跳动）</p> 
  - <p style="background-color:mediumseagreen;">&#x2003;问题二：对所有的参数更新时应用同样的learning rate，如果我们数据是稀疏的，我们更希望对出现频率低的特征进行大一点的更新。learning rate会随着更新的次数逐渐变小。</p> 

## 改进措施

  针对问题一（陷入鞍点或者发生震荡），提出Momentum动量，Nesterov Accelerated 加速的 Gradient

### 1. 动量Momentum的SGD
  - 使用Momentum动量的随机梯度下降法SGD主要思想是：引入一个积攒历史梯度信息来加速SGD。
  - Momentum优化表达式（与上述符号不一致）：
计算累计梯度 $$v_{t}=\alpha v_{t-1}+\eta_{t}\nabla J(W_{t}, X^{i},  Y^{i})$$
根据累计梯度更新参数 $$W_{t+1}=W_{t} - v_{t}$$
其中，$$v_{t}$$表示t时刻的累计梯度，$$W_{t}$$表示t时刻的模型参数，$$\alpha$$表示动力大小，$$\nabla J(W_{t}, X^{i}$$,  $$Y^{i})$$表示代价函数关于模型参数的偏倒数（梯度）。
  - 策略理解：
    - Momentum即动量，模拟物体运动时的惯性，即更新的时候再一定程度上保留之前更新的方向，同时利用当前batch的梯度微调最终的方向。
    - 策略使得梯度方向不变的维度上速度变快，梯度方向有所改变的维度上更新速度变慢，这样可以加快收敛并减小震荡。（前后梯度方向一致时,能够加速学习，前后梯度方向不一致时,能够抑制震荡）
  - 缺点：
    - 相当于小球从山上滚下来时是在盲目的沿着坡滚下，如果它能具备一些先知，例如快要上坡时，就知道需要减速的话，适应性更好。

### 2. 牛顿加速梯度 Nesterov Accelerated Gradient

  - Momentum动量算法的变种，参数更新规则：
       $$$v_{t}=\alpha v_{t-1}+\eta_{t}\nabla J(W_{t} - \alpha v_{t-1}) 
       W_{t+1}=W_{t}-v_{t}$$
     与标准的Momentum不同的是，先用当前的梯度更新一遍参数（$$W_{t} - \alpha v_{t-1}$$ 来近似当前参数下一步会变成的值），再用更新的临时参数计算梯度 $v_{t}$。
  - 策略理解
      - 计算梯度时，不在当前位置，而是在未来的位置上
      - 往标准动量中添加了一个校正因子
      - 在Momentum中小球会盲目的跟从下坡梯度，容易发生偏差。所以需要一个更聪明的小球，能提前知道它下一个位置大概在哪里。（提前知道下一个位置的梯度，然后使用到当前的参数更新中）
      - 在凸批量梯度的情况下，NAG将k步后额外的误差收敛率从 $$$O(1/k)$$ 改进到  $$O(1/k^{2})$$ ,然而在随机梯度情况下对收敛的作用却不是很大。

针对问题二（不同参数进行不同程度的更新），自适应学习率优化算法针对机器学习模型的学习率，传统的优化算法要么将学习率设置为常数要么根据训练次数调节学习率，极大的忽略了学习率的其它变化的可能性。然而，学习率对模型的性能有着显著的影响，因此要采取一些策略针对学习率进行改善，目前主要的自适应学习率优化算法主要包括：AdaGrad算法，RMSProp算法，Adam算法以及AdaDelta算法。花书《深度学习-8.5-自适应学习率算法》

### 1. AdaGrad算法 Adaptive Gradient Algorithm

  - 思想：
      - 独立适应模型所有参数的学习率，缩放每个参数反比于其所有梯度历史平方值总和的平方根。历史梯度较大的参数，学习率衰减的快，相应的历史梯度较小的参数学习率衰减的慢。
  - 更新规则：
       正确的参考《深度学习》的P188页
 以一个多分类问题解释，$$i$$表示第 $$i$$ 个分类， $$t$$ 表示迭代第 $$t$$ 次。$$\eta$$ 表示初始学习率，$$\epsilon$$ 是个很小的数保证分母不为0， $$W_{t+1,i}$$ 表示迭代第 t+1 次，第 i 个分类的参数，$$g_{(t,i)}$$ 表示第 t 次迭代时，分类i ，代价函数关于 $$W_{t,i}$$ 的梯度。
  -  从表达式中可以看出，对低频的参数做较大的更新，对高频的做较小的更新，正因为此，AdaGrad适用于稀疏数据以及不平衡数据，很好的提高了SGD的鲁棒性。
  - 优点：减少了学习率的手动调节
  - 缺点：分母不断的积累，学习率会越来越小。会导致有效学习率过早和过量的减小。在某些深度学习上效果不错，但不是全部。

### 2. RMSProp算法 Root Mean Square Prop

  - 思想：修改AdaGad的梯度累计为指数加权平均，指数衰减平均丢弃遥远过去的历史，使得其在非凸设定下效果更好。
  - 更新规则：参考《深度学习》的P188页
  - 引入参数衰减率控制移动平均的长度范围

### 3. AdaDelta算法
  - 在RMSProp基础上，不需要提前预设学习率（用额外的\Delta x 替代超参数）

### 4. Adam算法 Adaptive Moment Estimate
  - 通常Adam通常被认为是对超参数的选择相当鲁棒，尽管学习率有时需要从建议的默认修改。

## 总结
  - 原始优化方法存在问题：鞍点或局部最小值、所有参数都是一样的learning rate。
    - 针对鞍点或局部最小值问题，提出基于动量的方法，Momentum保留之前的梯度，NAG在Momentum基础上具备了先知（本次更新在预更新基础上进行）。
    - 针对所有参数共同学习率问题，提出利用梯度平方值总和平均缩小学习率的AdaGrad的方法对不平衡数据以及稀疏数据友好。针对AdaGrad算法的学习率衰减过快问题提出RMSProp和AdaDelta两个算法，RMSProp在AdaGrad基础上进行概率平方梯度累计（即设置衰减速率），AdaDelta在RMSProp基础上额外维护一个 $$\Delta x$$ 代替初始预设的学习率 $$\eta$$ 。除此之外，Adam在RMSprop基础上加了bias-correlation和momentum（参考《深度学习》的P189页，算法8.7）。
  - 如何选择
    - 如果数据稀疏，采用自适应的算法如AdaGrad、RMSProp、AdaDelta以及Adam算法。
    - 整体来讲Adam是最好的选择。
    - SGD虽然能够到达极小值，但是比其它算法用的时间长，而且可能别困在鞍点。
    - 如果需要更快的收敛，或者是训练更深更复杂的神经网络，需要用一种自适应的算法。

<p style="background-color:mediumseagreen;">.</p>
<font face='宋体' color='0099ff'>资源君</font>
参考以下blog进行总结，自适应优化算法需要参考花书《深度学习-8.5-自适应学习率算法》 ，不是参照以下博客的。
> https://www.cnblogs.com/guoyaohua/p/8542554.html
> https://blog.csdn.net/weixin_40170902/article/details/80092628（有动图）
> https://blog.csdn.net/weixin_29260031/article/details/82320525
