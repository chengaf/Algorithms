---
title: 梯度消失与梯度爆炸
date: 2019-10-09 11:44:43
tags: [深度学习,机器学习]
mathjax: true
---

&emsp;梯度消失（vanishing gradient）与梯度爆炸（exploding gradient）。根源是深度神经网络和反向传播。

> 为什么使用梯度更新规则
> 梯度消失和爆炸的原因
> 梯度消失和爆炸的解决方案
<!--more-->

### 1. 为什么使用梯度更新规则
&emsp;目前深度学习方法中，深度神经网络的发展造就了我们可以构造更深层的网络完成更复杂的任务，而且最终结果表明，在处理复杂任务上，深度网络比浅层的网络具有更好的效果。但是，目前优化神经网络的方法都是基于反向传播的思想，即根据损失函数计算的误差通过梯度反向传播的方式，指导深度网络权值的更新优化。这么做原因是因为求最优解是寻找最小值的优化问题，梯度下降再合适不过。

### 2. 梯度消失和爆炸的原因
&emsp;本质是一中情况。梯度消失一般因为两种原因：在**深层网络**中、**采用了不合适的损失函数**如sigmoid。梯度爆炸一般出现在在**深层网络**中、**权值初始化值太大**的情况。

  - 深层网络角度
    &emsp;以一个深层全连接网络(四层)为例，假设每一层激活函数的输出为 $$f_{i}(x)$$ , 其中 $$i$$ 为第 $$i$$ 层，$$x$$ 为第 $$i$$ 层的输入即第 $$i-1$$ 层的输出，$$f$$ 是激活函数。那么得出 $$f_{i+1}=f(w_{i+1}\cdot f_{i}+b_{i+1})$$ ,简单记为 $$f_{i+1}=f(w_{i+1}\cdot f_{i})$$ 
    &emsp;Back Propagation算法基于梯度下降策略，以目标的负梯度方向进行更新 $$w\gets w+\Delta w$$ ，给定学习率 $$\alpha$$ 得出 $$\Delta w=-\alpha \frac{\partial Loss}{\partial w}$$ 。如果需要更新第二层隐层的权值信息，根据链式法则，更新梯度信息：
 &emsp;&emsp;$$\Delta w_{2}=\frac{\partial Loss}{\partial w_{2}}=\frac{\partial Loss}{\partial f_{4}}\frac{\partial f_{4}}{\partial f_{3}}\frac{\partial f_{3}}{\partial f_{2}}\frac{\partial f_{2}}{\partial w_{2}}$$

  - 激活函数角度
  ......

<p style="background-color:mediumseagreen;">.</p>
<font face='宋体' color='0099ff'>资源君</font>
> 1. https://blog.csdn.net/qq_25737169/article/details/78847691
> 2. http://www.pianshen.com/article/766173300/
> 3. https://www.jianshu.com/p/243ab5aff906